{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Titanic Dataframe analysis</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import das bibliotecas utilizadas</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Importação de dados</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Sandstrom, Miss. Marguerite Rut</td>\n",
       "      <td>female</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PP 9549</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>G6</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bonnell, Miss. Elizabeth</td>\n",
       "      <td>female</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113783</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>C103</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Saundercock, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5. 2151</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Andersson, Mr. Anders Johan</td>\n",
       "      <td>male</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>347082</td>\n",
       "      <td>31.2750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Vestrom, Miss. Hulda Amanda Adolfina</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350406</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PassengerId  Survived  Pclass  \\\n",
       "0             1         0       3   \n",
       "1             2         1       1   \n",
       "2             3         1       3   \n",
       "3             4         1       1   \n",
       "4             5         0       3   \n",
       "5             6         0       3   \n",
       "6             7         0       1   \n",
       "7             8         0       3   \n",
       "8             9         1       3   \n",
       "9            10         1       2   \n",
       "10           11         1       3   \n",
       "11           12         1       1   \n",
       "12           13         0       3   \n",
       "13           14         0       3   \n",
       "14           15         0       3   \n",
       "\n",
       "                                                 Name     Sex   Age  SibSp  \\\n",
       "0                             Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1   Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                              Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3        Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                            Allen, Mr. William Henry    male  35.0      0   \n",
       "5                                    Moran, Mr. James    male   NaN      0   \n",
       "6                             McCarthy, Mr. Timothy J    male  54.0      0   \n",
       "7                      Palsson, Master. Gosta Leonard    male   2.0      3   \n",
       "8   Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
       "9                 Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
       "10                    Sandstrom, Miss. Marguerite Rut  female   4.0      1   \n",
       "11                           Bonnell, Miss. Elizabeth  female  58.0      0   \n",
       "12                     Saundercock, Mr. William Henry    male  20.0      0   \n",
       "13                        Andersson, Mr. Anders Johan    male  39.0      1   \n",
       "14               Vestrom, Miss. Hulda Amanda Adolfina  female  14.0      0   \n",
       "\n",
       "    Parch            Ticket     Fare Cabin Embarked  \n",
       "0       0         A/5 21171   7.2500   NaN        S  \n",
       "1       0          PC 17599  71.2833   C85        C  \n",
       "2       0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3       0            113803  53.1000  C123        S  \n",
       "4       0            373450   8.0500   NaN        S  \n",
       "5       0            330877   8.4583   NaN        Q  \n",
       "6       0             17463  51.8625   E46        S  \n",
       "7       1            349909  21.0750   NaN        S  \n",
       "8       2            347742  11.1333   NaN        S  \n",
       "9       0            237736  30.0708   NaN        C  \n",
       "10      1           PP 9549  16.7000    G6        S  \n",
       "11      0            113783  26.5500  C103        S  \n",
       "12      0         A/5. 2151   8.0500   NaN        S  \n",
       "13      5            347082  31.2750   NaN        S  \n",
       "14      0            350406   7.8542   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tratamento dos dados</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tratamento do Nome - Criação da coluna Title</h3>\n",
    "\n",
    "Nessa parte, foram retirados os títulos dos nomes e agregados em categorias parecidas. Isso foi feito para um melhor preenchemento das idades faltantes. Além disso, essa nova coluna possui, além da divisão de homens e mulheres por idade, titulos de nobreza e oficiais, possibilitando possíveis hipóteses sobre a sobrevivência dessas categorias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separa_titulo(nome):\n",
    "    titulo = nome.split(\",\")[1].split(\".\")[0].strip()\n",
    "    return titulo\n",
    "\n",
    "normalized_titles = {\n",
    "    \"Capt\":       \"Officer\",\n",
    "    \"Col\":        \"Officer\",\n",
    "    \"Major\":      \"Officer\",\n",
    "    \"Jonkheer\":   \"Royalty\",\n",
    "    \"Don\":        \"Royalty\",\n",
    "    \"Sir\" :       \"Royalty\",\n",
    "    \"Dr\":         \"Officer\",\n",
    "    \"Rev\":        \"Officer\",\n",
    "    \"the Countess\":\"Royalty\",\n",
    "    \"Dona\":       \"Royalty\",\n",
    "    \"Mme\":        \"Mrs\",\n",
    "    \"Mlle\":       \"Miss\",\n",
    "    \"Ms\":         \"Mrs\",\n",
    "    \"Mr\" :        \"Mr\",\n",
    "    \"Mrs\" :       \"Mrs\",\n",
    "    \"Miss\" :      \"Miss\",\n",
    "    \"Master\" :    \"Master\",\n",
    "    \"Lady\" :      \"Royalty\"\n",
    "}\n",
    "df['Title'] = df['Name'].apply(separa_titulo)\n",
    "df[\"Title\"] = df[\"Title\"].map(normalized_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title\n",
       "Master       4\n",
       "Miss        36\n",
       "Mr         119\n",
       "Mrs         17\n",
       "Officer      1\n",
       "Name: PassengerId, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Age\"].isnull()].groupby([\"Title\"]).count()[\"PassengerId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.0 22.0 5.0\n"
     ]
    }
   ],
   "source": [
    "adults_age = np.round(df[df[\"Title\"].isin([\"Mr\",\"Mrs\",\"Officer\",\"Royalty\"])][\"Age\"].mean())\n",
    "miss_age = np.round(df[df[\"Title\"].isin([\"Miss\"])][\"Age\"].mean())\n",
    "master_age = np.round(df[df[\"Title\"].isin([\"Master\"])][\"Age\"].mean())\n",
    "print(adults_age,miss_age,master_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"Title\"].isin([\"Mr\",\"Mrs\",\"Officer\",\"Royalty\"]) & df[\"Age\"].isnull(),\"Age\"] = adults_age\n",
    "df.loc[df[\"Title\"].isin([\"Miss\"]) & df[\"Age\"].isnull(),\"Age\"]= miss_age\n",
    "df.loc[df[\"Title\"].isin([\"Master\"]) & df[\"Age\"].isnull(),\"Age\"] = master_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Retirando colunas desnecessárias que não serão utilizadas no modelo</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"Name\",\"PassengerId\",\"Ticket\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tratamento da coluna Cabin</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cabin'].fillna('Missing',inplace=True)\n",
    "df['Cabin'] = df['Cabin'].str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Criação de uma nova coluna com o tamanho da familia</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"FamSize\"] = df[\"SibSp\"] + df[\"Parch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preenchimento dos valores faltantes da coluna Embarked</h3>\n",
    "\n",
    "Primeiro foi verificado qual era o valor que mais aparecia e os valores faltantes foram preenchidos com tal valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S    644\n",
       "C    168\n",
       "Q     77\n",
       "Name: Embarked, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Embarked\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Embarked\"].fillna('S',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Transformação colunas categoricas + Label encoder</h3>\n",
    "\n",
    "Primeiro foram modificados os tipos dessas colunas para categórico, e então utilizado o LabelEnconder para transformar os valores em números"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Pclass'] = df['Pclass'].astype('category')\n",
    "# df['Sex'] = df['Sex'].astype('category')\n",
    "# df['Embarked'] = df['Embarked'].astype('category')\n",
    "# df['Title'] = df['Title'].astype('category')\n",
    "# df['Cabin'] = df['Cabin'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelencoder = LabelEncoder()\n",
    "\n",
    "# df['Pclass'] = labelencoder.fit_transform(df['Pclass'])\n",
    "# df['Sex'] = labelencoder.fit_transform(df['Sex'])\n",
    "# df['Title'] = labelencoder.fit_transform(df['Title'])\n",
    "# df['Embarked'] = labelencoder.fit_transform(df['Embarked'])\n",
    "# df['Cabin'] = labelencoder.fit_transform(df['Cabin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>One Hot Encoder</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "coluna = ['Embarked', 'Title','Pclass','Sex','Cabin']\n",
    "df = pd.get_dummies(df, columns=coluna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Normalização da coluna Fare</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[  7.25    71.2833   7.925   53.1      8.05     8.4583  51.8625  21.075\n  11.1333  30.0708  16.7     26.55     8.05    31.275    7.8542  16.\n  29.125   13.      18.       7.225   26.      13.       8.0292  35.5\n  21.075   31.3875   7.225  263.       7.8792   7.8958  27.7208 146.5208\n   7.75    10.5     82.1708  52.       7.2292   8.05    18.      11.2417\n   9.475   21.       7.8958  41.5792   7.8792   8.05    15.5      7.75\n  21.6792  17.8     39.6875   7.8     76.7292  26.      61.9792  35.5\n  10.5      7.2292  27.75    46.9      7.2292  80.      83.475   27.9\n  27.7208  15.2458  10.5      8.1583   7.925    8.6625  10.5     46.9\n  73.5     14.4542  56.4958   7.65     7.8958   8.05    29.      12.475\n   9.       9.5      7.7875  47.1     10.5     15.85    34.375    8.05\n 263.       8.05     8.05     7.8542  61.175   20.575    7.25     8.05\n  34.6542  63.3583  23.      26.       7.8958   7.8958  77.2875   8.6542\n   7.925    7.8958   7.65     7.775    7.8958  24.15    52.      14.4542\n   8.05     9.825   14.4583   7.925    7.75    21.     247.5208  31.275\n  73.5      8.05    30.0708  13.      77.2875  11.2417   7.75     7.1417\n  22.3583   6.975    7.8958   7.05    14.5     26.      13.      15.0458\n  26.2833  53.1      9.2167  79.2     15.2458   7.75    15.85     6.75\n  11.5     36.75     7.7958  34.375   26.      13.      12.525   66.6\n   8.05    14.5      7.3125  61.3792   7.7333   8.05     8.6625  69.55\n  16.1     15.75     7.775    8.6625  39.6875  20.525   55.      27.9\n  25.925   56.4958  33.5     29.125   11.1333   7.925   30.6958   7.8542\n  25.4667  28.7125  13.       0.      69.55    15.05    31.3875  39.\n  22.025   50.      15.5     26.55    15.5      7.8958  13.      13.\n   7.8542  26.      27.7208 146.5208   7.75     8.4042   7.75    13.\n   9.5     69.55     6.4958   7.225    8.05    10.4625  15.85    18.7875\n   7.75    31.       7.05    21.       7.25    13.       7.75   113.275\n   7.925   27.      76.2917  10.5      8.05    13.       8.05     7.8958\n  90.       9.35    10.5      7.25    13.      25.4667  83.475    7.775\n  13.5     31.3875  10.5      7.55    26.      26.25    10.5     12.275\n  14.4542  15.5     10.5      7.125    7.225   90.       7.775   14.5\n  52.5542  26.       7.25    10.4625  26.55    16.1     20.2125  15.2458\n  79.2     86.5    512.3292  26.       7.75    31.3875  79.65     0.\n   7.75    10.5     39.6875   7.775  153.4625 135.6333  31.       0.\n  19.5     29.7      7.75    77.9583   7.75     0.      29.125   20.25\n   7.75     7.8542   9.5      8.05    26.       8.6625   9.5      7.8958\n  13.       7.75    78.85    91.0792  12.875    8.85     7.8958  27.7208\n   7.2292 151.55    30.5    247.5208   7.75    23.25     0.      12.35\n   8.05   151.55   110.8833 108.9     24.      56.9292  83.1583 262.375\n  26.       7.8958  26.25     7.8542  26.      14.     164.8667 134.5\n   7.25     7.8958  12.35    29.      69.55   135.6333   6.2375  13.\n  20.525   57.9792  23.25    28.5    153.4625  18.     133.65     7.8958\n  66.6    134.5      8.05    35.5     26.     263.      13.      13.\n  13.      13.      13.      16.1     15.9      8.6625   9.225   35.\n   7.2292  17.8      7.225    9.5     55.      13.       7.8792   7.8792\n  27.9     27.7208  14.4542   7.05    15.5      7.25    75.25     7.2292\n   7.75    69.3     55.4417   6.4958   8.05   135.6333  21.075   82.1708\n   7.25   211.5      4.0125   7.775  227.525   15.7417   7.925   52.\n   7.8958  73.5     46.9     13.       7.7292  12.     120.       7.7958\n   7.925  113.275   16.7      7.7958   7.8542  26.      10.5     12.65\n   7.925    8.05     9.825   15.85     8.6625  21.       7.75    18.75\n   7.775   25.4667   7.8958   6.8583  90.       0.       7.925    8.05\n  32.5     13.      13.      24.15     7.8958   7.7333   7.875   14.4\n  20.2125   7.25    26.      26.       7.75     8.05    26.55    16.1\n  26.       7.125   55.9    120.      34.375   18.75   263.      10.5\n  26.25     9.5      7.775   13.       8.1125  81.8583  19.5     26.55\n  19.2583  30.5     27.75    19.9667  27.75    89.1042   8.05     7.8958\n  26.55    51.8625  10.5      7.75    26.55     8.05    38.5     13.\n   8.05     7.05     0.      26.55     7.725   19.2583   7.25     8.6625\n  27.75    13.7917   9.8375  52.      21.       7.0458   7.5208  12.2875\n  46.9      0.       8.05     9.5875  91.0792  25.4667  90.      29.7\n   8.05    15.9     19.9667   7.25    30.5     49.5042   8.05    14.4583\n  78.2667  15.1    151.55     7.7958   8.6625   7.75     7.6292   9.5875\n  86.5    108.9     26.      26.55    22.525   56.4958   7.75     8.05\n  26.2875  59.4      7.4958  34.0208  10.5     24.15    26.       7.8958\n  93.5      7.8958   7.225   57.9792   7.2292   7.75    10.5    221.7792\n   7.925   11.5     26.       7.2292   7.2292  22.3583   8.6625  26.25\n  26.55   106.425   14.5     49.5     71.      31.275   31.275   26.\n 106.425   26.      26.      13.8625  20.525   36.75   110.8833  26.\n   7.8292   7.225    7.775   26.55    39.6    227.525   79.65    17.4\n   7.75     7.8958  13.5      8.05     8.05    24.15     7.8958  21.075\n   7.2292   7.8542  10.5     51.4792  26.3875   7.75     8.05    14.5\n  13.      55.9     14.4583   7.925   30.     110.8833  26.      40.125\n   8.7125  79.65    15.      79.2      8.05     8.05     7.125   78.2667\n   7.25     7.75    26.      24.15    33.       0.       7.225   56.9292\n  27.       7.8958  42.4      8.05    26.55    15.55     7.8958  30.5\n  41.5792 153.4625  31.275    7.05    15.5      7.75     8.05    65.\n  14.4     16.1     39.      10.5     14.4542  52.5542  15.7417   7.8542\n  16.1     32.3208  12.35    77.9583   7.8958   7.7333  30.       7.0542\n  30.5      0.      27.9     13.       7.925   26.25    39.6875  16.1\n   7.8542  69.3     27.9     56.4958  19.2583  76.7292   7.8958  35.5\n   7.55     7.55     7.8958  23.       8.4333   7.8292   6.75    73.5\n   7.8958  15.5     13.     113.275  133.65     7.225   25.5875   7.4958\n   7.925   73.5     13.       7.775    8.05    52.      39.      52.\n  10.5     13.       0.       7.775    8.05     9.8417  46.9    512.3292\n   8.1375  76.7292   9.225   46.9     39.      41.5792  39.6875  10.1708\n   7.7958 211.3375  57.      13.4167  56.4958   7.225   26.55    13.5\n   8.05     7.7333 110.8833   7.65   227.525   26.2875  14.4542   7.7417\n   7.8542  26.      13.5     26.2875 151.55    15.2458  49.5042  26.55\n  52.       9.4833  13.       7.65   227.525   10.5     15.5      7.775\n  33.       7.0542  13.      13.      53.1      8.6625  21.       7.7375\n  26.       7.925  211.3375  18.7875   0.      13.      13.      16.1\n  34.375  512.3292   7.8958   7.8958  30.      78.85   262.375   16.1\n   7.925   71.      20.25    13.      53.1      7.75    23.      12.475\n   9.5      7.8958  65.      14.5      7.7958  11.5      8.05    86.5\n  14.5      7.125    7.2292 120.       7.775   77.9583  39.6      7.75\n  24.15     8.3625   9.5      7.8542  10.5      7.225   23.       7.75\n   7.75    12.475    7.7375 211.3375   7.2292  57.      30.      23.45\n   7.05     7.25     7.4958  29.125   20.575   79.2      7.75    26.\n  69.55    30.6958   7.8958  13.      25.9292   8.6833   7.2292  24.15\n  13.      26.25   120.       8.5167   6.975    7.775    0.       7.775\n  13.      53.1      7.8875  24.15    10.5     31.275    8.05     0.\n   7.925   37.0042   6.45    27.9     93.5      8.6625   0.      12.475\n  39.6875   6.95    56.4958  37.0042   7.75    80.      14.4542  18.75\n   7.2292   7.8542   8.3     83.1583   8.6625   8.05    56.4958  29.7\n   7.925   10.5     31.       6.4375   8.6625   7.55    69.55     7.8958\n  33.      89.1042  31.275    7.775   15.2458  39.4     26.       9.35\n 164.8667  26.55    19.2583   7.2292  14.1083  11.5     25.9292  69.55\n  13.      13.      13.8583  50.4958   9.5     11.1333   7.8958  52.5542\n   5.       9.      24.       7.225    9.8458   7.8958   7.8958  83.1583\n  26.       7.8958  10.5167  10.5      7.05    29.125   13.      30.\n  23.45    30.       7.75  ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-c9e2b77a3747>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Fare'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Fare'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    694\u001b[0m             \u001b[0mTransformer\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \"\"\"\n\u001b[1;32m--> 696\u001b[1;33m         X = self._validate_data(X, accept_sparse=('csr', 'csc'),\n\u001b[0m\u001b[0;32m    697\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m                                 force_all_finite='allow-nan')\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m                     \u001b[1;34mf\"requires y to be passed, but the target y is None.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m                 )\n\u001b[1;32m--> 420\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    617\u001b[0m             \u001b[1;31m# If input is 1D raise error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    620\u001b[0m                     \u001b[1;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[  7.25    71.2833   7.925   53.1      8.05     8.4583  51.8625  21.075\n  11.1333  30.0708  16.7     26.55     8.05    31.275    7.8542  16.\n  29.125   13.      18.       7.225   26.      13.       8.0292  35.5\n  21.075   31.3875   7.225  263.       7.8792   7.8958  27.7208 146.5208\n   7.75    10.5     82.1708  52.       7.2292   8.05    18.      11.2417\n   9.475   21.       7.8958  41.5792   7.8792   8.05    15.5      7.75\n  21.6792  17.8     39.6875   7.8     76.7292  26.      61.9792  35.5\n  10.5      7.2292  27.75    46.9      7.2292  80.      83.475   27.9\n  27.7208  15.2458  10.5      8.1583   7.925    8.6625  10.5     46.9\n  73.5     14.4542  56.4958   7.65     7.8958   8.05    29.      12.475\n   9.       9.5      7.7875  47.1     10.5     15.85    34.375    8.05\n 263.       8.05     8.05     7.8542  61.175   20.575    7.25     8.05\n  34.6542  63.3583  23.      26.       7.8958   7.8958  77.2875   8.6542\n   7.925    7.8958   7.65     7.775    7.8958  24.15    52.      14.4542\n   8.05     9.825   14.4583   7.925    7.75    21.     247.5208  31.275\n  73.5      8.05    30.0708  13.      77.2875  11.2417   7.75     7.1417\n  22.3583   6.975    7.8958   7.05    14.5     26.      13.      15.0458\n  26.2833  53.1      9.2167  79.2     15.2458   7.75    15.85     6.75\n  11.5     36.75     7.7958  34.375   26.      13.      12.525   66.6\n   8.05    14.5      7.3125  61.3792   7.7333   8.05     8.6625  69.55\n  16.1     15.75     7.775    8.6625  39.6875  20.525   55.      27.9\n  25.925   56.4958  33.5     29.125   11.1333   7.925   30.6958   7.8542\n  25.4667  28.7125  13.       0.      69.55    15.05    31.3875  39.\n  22.025   50.      15.5     26.55    15.5      7.8958  13.      13.\n   7.8542  26.      27.7208 146.5208   7.75     8.4042   7.75    13.\n   9.5     69.55     6.4958   7.225    8.05    10.4625  15.85    18.7875\n   7.75    31.       7.05    21.       7.25    13.       7.75   113.275\n   7.925   27.      76.2917  10.5      8.05    13.       8.05     7.8958\n  90.       9.35    10.5      7.25    13.      25.4667  83.475    7.775\n  13.5     31.3875  10.5      7.55    26.      26.25    10.5     12.275\n  14.4542  15.5     10.5      7.125    7.225   90.       7.775   14.5\n  52.5542  26.       7.25    10.4625  26.55    16.1     20.2125  15.2458\n  79.2     86.5    512.3292  26.       7.75    31.3875  79.65     0.\n   7.75    10.5     39.6875   7.775  153.4625 135.6333  31.       0.\n  19.5     29.7      7.75    77.9583   7.75     0.      29.125   20.25\n   7.75     7.8542   9.5      8.05    26.       8.6625   9.5      7.8958\n  13.       7.75    78.85    91.0792  12.875    8.85     7.8958  27.7208\n   7.2292 151.55    30.5    247.5208   7.75    23.25     0.      12.35\n   8.05   151.55   110.8833 108.9     24.      56.9292  83.1583 262.375\n  26.       7.8958  26.25     7.8542  26.      14.     164.8667 134.5\n   7.25     7.8958  12.35    29.      69.55   135.6333   6.2375  13.\n  20.525   57.9792  23.25    28.5    153.4625  18.     133.65     7.8958\n  66.6    134.5      8.05    35.5     26.     263.      13.      13.\n  13.      13.      13.      16.1     15.9      8.6625   9.225   35.\n   7.2292  17.8      7.225    9.5     55.      13.       7.8792   7.8792\n  27.9     27.7208  14.4542   7.05    15.5      7.25    75.25     7.2292\n   7.75    69.3     55.4417   6.4958   8.05   135.6333  21.075   82.1708\n   7.25   211.5      4.0125   7.775  227.525   15.7417   7.925   52.\n   7.8958  73.5     46.9     13.       7.7292  12.     120.       7.7958\n   7.925  113.275   16.7      7.7958   7.8542  26.      10.5     12.65\n   7.925    8.05     9.825   15.85     8.6625  21.       7.75    18.75\n   7.775   25.4667   7.8958   6.8583  90.       0.       7.925    8.05\n  32.5     13.      13.      24.15     7.8958   7.7333   7.875   14.4\n  20.2125   7.25    26.      26.       7.75     8.05    26.55    16.1\n  26.       7.125   55.9    120.      34.375   18.75   263.      10.5\n  26.25     9.5      7.775   13.       8.1125  81.8583  19.5     26.55\n  19.2583  30.5     27.75    19.9667  27.75    89.1042   8.05     7.8958\n  26.55    51.8625  10.5      7.75    26.55     8.05    38.5     13.\n   8.05     7.05     0.      26.55     7.725   19.2583   7.25     8.6625\n  27.75    13.7917   9.8375  52.      21.       7.0458   7.5208  12.2875\n  46.9      0.       8.05     9.5875  91.0792  25.4667  90.      29.7\n   8.05    15.9     19.9667   7.25    30.5     49.5042   8.05    14.4583\n  78.2667  15.1    151.55     7.7958   8.6625   7.75     7.6292   9.5875\n  86.5    108.9     26.      26.55    22.525   56.4958   7.75     8.05\n  26.2875  59.4      7.4958  34.0208  10.5     24.15    26.       7.8958\n  93.5      7.8958   7.225   57.9792   7.2292   7.75    10.5    221.7792\n   7.925   11.5     26.       7.2292   7.2292  22.3583   8.6625  26.25\n  26.55   106.425   14.5     49.5     71.      31.275   31.275   26.\n 106.425   26.      26.      13.8625  20.525   36.75   110.8833  26.\n   7.8292   7.225    7.775   26.55    39.6    227.525   79.65    17.4\n   7.75     7.8958  13.5      8.05     8.05    24.15     7.8958  21.075\n   7.2292   7.8542  10.5     51.4792  26.3875   7.75     8.05    14.5\n  13.      55.9     14.4583   7.925   30.     110.8833  26.      40.125\n   8.7125  79.65    15.      79.2      8.05     8.05     7.125   78.2667\n   7.25     7.75    26.      24.15    33.       0.       7.225   56.9292\n  27.       7.8958  42.4      8.05    26.55    15.55     7.8958  30.5\n  41.5792 153.4625  31.275    7.05    15.5      7.75     8.05    65.\n  14.4     16.1     39.      10.5     14.4542  52.5542  15.7417   7.8542\n  16.1     32.3208  12.35    77.9583   7.8958   7.7333  30.       7.0542\n  30.5      0.      27.9     13.       7.925   26.25    39.6875  16.1\n   7.8542  69.3     27.9     56.4958  19.2583  76.7292   7.8958  35.5\n   7.55     7.55     7.8958  23.       8.4333   7.8292   6.75    73.5\n   7.8958  15.5     13.     113.275  133.65     7.225   25.5875   7.4958\n   7.925   73.5     13.       7.775    8.05    52.      39.      52.\n  10.5     13.       0.       7.775    8.05     9.8417  46.9    512.3292\n   8.1375  76.7292   9.225   46.9     39.      41.5792  39.6875  10.1708\n   7.7958 211.3375  57.      13.4167  56.4958   7.225   26.55    13.5\n   8.05     7.7333 110.8833   7.65   227.525   26.2875  14.4542   7.7417\n   7.8542  26.      13.5     26.2875 151.55    15.2458  49.5042  26.55\n  52.       9.4833  13.       7.65   227.525   10.5     15.5      7.775\n  33.       7.0542  13.      13.      53.1      8.6625  21.       7.7375\n  26.       7.925  211.3375  18.7875   0.      13.      13.      16.1\n  34.375  512.3292   7.8958   7.8958  30.      78.85   262.375   16.1\n   7.925   71.      20.25    13.      53.1      7.75    23.      12.475\n   9.5      7.8958  65.      14.5      7.7958  11.5      8.05    86.5\n  14.5      7.125    7.2292 120.       7.775   77.9583  39.6      7.75\n  24.15     8.3625   9.5      7.8542  10.5      7.225   23.       7.75\n   7.75    12.475    7.7375 211.3375   7.2292  57.      30.      23.45\n   7.05     7.25     7.4958  29.125   20.575   79.2      7.75    26.\n  69.55    30.6958   7.8958  13.      25.9292   8.6833   7.2292  24.15\n  13.      26.25   120.       8.5167   6.975    7.775    0.       7.775\n  13.      53.1      7.8875  24.15    10.5     31.275    8.05     0.\n   7.925   37.0042   6.45    27.9     93.5      8.6625   0.      12.475\n  39.6875   6.95    56.4958  37.0042   7.75    80.      14.4542  18.75\n   7.2292   7.8542   8.3     83.1583   8.6625   8.05    56.4958  29.7\n   7.925   10.5     31.       6.4375   8.6625   7.55    69.55     7.8958\n  33.      89.1042  31.275    7.775   15.2458  39.4     26.       9.35\n 164.8667  26.55    19.2583   7.2292  14.1083  11.5     25.9292  69.55\n  13.      13.      13.8583  50.4958   9.5     11.1333   7.8958  52.5542\n   5.       9.      24.       7.225    9.8458   7.8958   7.8958  83.1583\n  26.       7.8958  10.5167  10.5      7.05    29.125   13.      30.\n  23.45    30.       7.75  ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# def minmax_norm(column):\n",
    "#     return (column - column.min()) / (column.max() - column.min())\n",
    "\n",
    "# df['Fare'] = minmax_norm(df['Fare'])\n",
    "# df['Age'] = minmax_norm(df['Age'])\n",
    "def padroniza(column):\n",
    "        return ((column - column.mean())/column.std())\n",
    "df['Fare'] = padroniza(df['Fare'])\n",
    "df['Age'] = padroniza(df['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tratamento da base de testes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Title'] = df_test['Name'].apply(separa_titulo)\n",
    "df_test[\"Title\"] = df_test[\"Title\"].map(normalized_titles)\n",
    "\n",
    "df_test.loc[df_test[\"Title\"].isin([\"Mr\",\"Mrs\",\"Officer\",\"Royalty\"]) & df_test[\"Age\"].isnull(),\"Age\"] = adults_age\n",
    "df_test.loc[df_test[\"Title\"].isin([\"Miss\"]) & df_test[\"Age\"].isnull(),\"Age\"]= miss_age\n",
    "df_test.loc[df_test[\"Title\"].isin([\"Master\"]) & df_test[\"Age\"].isnull(),\"Age\"] = master_age\n",
    "\n",
    "passengers = df_test[\"PassengerId\"]\n",
    "\n",
    "df_test.drop(columns=[\"Name\",\"PassengerId\",\"Ticket\"],inplace=True)\n",
    "\n",
    "df_test['Cabin'].fillna('Missing',inplace=True)\n",
    "df_test['Cabin'] = df_test['Cabin'].str[0]\n",
    "\n",
    "df_test[\"FamSize\"] = df_test[\"SibSp\"] + df_test[\"Parch\"]\n",
    "\n",
    "df_test[\"Embarked\"].fillna('S',inplace=True)\n",
    "\n",
    "# df_test['Pclass'] = df_test['Pclass'].astype('category')\n",
    "# df_test['Sex'] = df_test['Sex'].astype('category')\n",
    "# df_test['Embarked'] = df_test['Embarked'].astype('category')\n",
    "# df_test['Title'] = df_test['Title'].astype('category')\n",
    "# df_test['Cabin'] = df_test['Cabin'].astype('category')\n",
    "\n",
    "# df_test['Pclass'] = labelencoder.fit_transform(df_test['Pclass'])\n",
    "# df_test['Sex'] = labelencoder.fit_transform(df_test['Sex'])\n",
    "# df_test['Title'] = labelencoder.fit_transform(df_test['Title'])\n",
    "# df_test['Embarked'] = labelencoder.fit_transform(df_test['Embarked'])\n",
    "# df_test['Cabin'] = labelencoder.fit_transform(df_test['Cabin'])\n",
    "\n",
    "coluna = ['Embarked', 'Title','Pclass','Sex','Cabin']\n",
    "df_test = pd.get_dummies(df_test, columns=coluna)\n",
    "\n",
    "# df_test['Fare'] = minmax_norm(df_test['Fare'])\n",
    "# df['Age'] = minmax_norm(df['Age'])\n",
    "df_test[\"Fare\"].fillna(df_test[\"Fare\"].mean(),inplace=True)\n",
    "df['Fare'] = scaler.fit_transform(df['Fare'])\n",
    "df['Age'] = scaler.fit_transform(df['Age'])\n",
    "\n",
    "df_test[\"Cabin_T\"] = 0\n",
    "df_test.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Criação e teste do modelo</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = [\"Survived\"])\n",
    "Y = df[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=200)\n",
    "logreg.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "logreg.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Aplicação do modelo para submissão no kaggle</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Resultado = logreg.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'PassengerId': passengers, 'Survived': Resultado}\n",
    "tabfinal = pd.DataFrame(data=d)\n",
    "tabfinal.set_index(\"PassengerId\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabfinal.to_csv(\"ResultadoKaggle.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
